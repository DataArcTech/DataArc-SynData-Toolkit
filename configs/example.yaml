# global configuration
seed: 2024
# CUDA device for all GPU operations (vLLM, SentenceTransformer, MinerU)
device: cuda:0
# synthetic dataset output directory. Data will save in {output_dir}/{task_name}_final.{export_format}
output_dir: ./outputs/gsm8k
# export dataset format
export_format: jsonl

# Global task configuration
task:
  name: gsm8k
  domain: mathematics
  demo_examples_path: /path/to/your/demo_examples.jsonl  # path for demo examples .jsonl file (Optional)
  task_type: local
  
  # local task configuration
  local:
    # Parsing Configuration
    parsing:
      # Directions to document corpora
      document_dir: /path/to/your/document_dir
      method: mineru  # default to mineru parser

    # Retrieval Configuration
    retrieval:
      passages_dir: ./dataset/passages   # path to document passages after parsing and chunking
      method: bm25       # default to bm25 retriever
      top_k: 1000        # selected top k passage retrieved
    generation:
      # Task instruction used for data generation
      task_instruction: |
        Generate a grade school math word problem that requires multi-step reasoning.
        The problem should involve basic arithmetic operations and have a clear numerical answer.
      # Input instruction - prepended to model input during inference
      input_instruction: |
        The input should be a math word problem that requires multi-step reasoning.
      # Output instruction - appended to prompt to guide output format (HOW to solve)
      output_instruction: |
        The output should contain reasoning process step by step.
      num_samples: 10        # number of initial data
      temperature: 1.0       # temperature for LLM in initial data generation

  # web task configuration
  web:
    huggingface_token: "hf_"        # optional. This parameter can be set by 'export HUGGINGFACE_TOKEN="YOUR TOKEN"'
    task_instruction: |
      Generate a grade school math word problem that requires multi-step reasoning.
      The problem should involve basic arithmetic operations and have a clear numerical answer.
    input_instruction: |
      The input should be a math word problem that requires multi-step reasoning.
    output_instruction: |
      The output should contain reasoning process step by step.
    num_samples: 10              # number of initial data
    dataset_score_threshold: 30  # minimum overall score (sum of 5 criteria) for a dataset to be valid
    
              
  # distill task configuration
  distill:
    # Task instruction for synthetic data generation
    task_instruction: |
      Generate a grade school math word problem that requires multi-step reasoning.
      The problem should involve basic arithmetic operations and have a clear numerical answer.
    # Input instruction - prepended to model input during inference
    input_instruction: |
      The input should be a math word problem that requires multi-step reasoning.
    # Output instruction - appended to prompt to guide output format
    output_instruction: |
      The output should contain reasoning process step by step.
    # Number of synthetic samples to generate
    num_samples: 10
    # Batch size for generation (how many samples to generate per API call)
    batch_size: 5
    # Temperature for LLM generation
    temperature: 1.0


# Base model for evaluation and inference load from vllm
base_model:
  # Base model path
  path: Qwen/Qwen2.5-7B
  # Inference parameters for evaluation
  inference:
    temperature: 0.0
    max_tokens: 1500
    top_p: 0.95
    n: 1
  # Sampling parameters for scoring (pass@n evaluation)
  scoring:
    temperature: 1.2
    n: 8

# LLM used throughout the pipeline (Create .env file to specified base url and api key)
llm:
  provider: openai
  model: gpt-4o-mini

# Answer extraction configuration (HOW to mark final answer)
answer_extraction:
  tag: "####"
  instruction: "Output your final answer after ####"

# Postprocess for LLMs' responses
postprocess:
  methods: 
  - "majority_voting"
      
  # Majority voting for quality control of LLM's responses
  majority_voting:
    # Number of voting
    n: 8
    # Method: "exact_match", "semantic_clustering", "llm_judge"
    method: exact_match
    # Exact match settings (only used if method is "exact_match")
    exact_match:
      # For numeric answers, use tolerance instead of strict equality
      numeric_tolerance: 1e-3  # Aligned with Data_Synthesis_RL gsm8k, set to 0 or null for strict string matching
    # # Semantic similarity settings (only used if method is "semantic_clustering")
    # semantic_clustering:
    #   model_path: BAAI/bge-large-zh-v1.5
    #   similarity_threshold: 0.85
    # # LLM judge settings (only used if method is "llm_judge")
    # llm_judge:
    #     temperature: 0.3

# Evaluation Configuration
evaluation:
  # Batch size for evaluation
  batch_size: 5

  # Answer comparison configuration (HOW to compare two answers)
  # Used for both evaluation scoring and majority voting
  answer_comparison:
    # Method: "exact_match", "semantic", "llm_judge"
    method: semantic

    # # Exact match settings (only used if method is "exact_match")
    # exact_match:
    #   # For numeric answers, use tolerance instead of strict equality
    #   numeric_tolerance: 1e-3  # Aligned with Data_Synthesis_RL gsm8k, set to 0 or null for strict string matching

    # Semantic similarity settings (only used if method is "semantic")
    semantic:
      model_path: BAAI/bge-small-en-v1.5
      similarity_threshold: 0.85

    # # LLM judge settings (only used if method is "llm_judge")
    # llm_judge:
    #   temperature: 0.3

# Rewrite Configuration (Optional)
rewrite:
  method: "difficulty_adjust"
  input_instruction: |
      Solve the following math problem step by step. Show your work.
  output_instruction: |
      Let's think step by step.

  difficulty_adjust:
    easier_temperature: 0.9
    harder_temperature: 1.1

# Translation Configuration
# If language is not 'english', you must specify model_path for the translation model
translation:
  language: english  # Target language: 'english' (no translation), 'arabic', etc.
  # model_path: "path/to/your/Translator"  
  max_tokens: 256    # Maximum tokens for translation generation
  batch_size: 1      # Batch size for translation